# ðŸ§  IRIS Vision Integration Plan â€” MiniCPM-o 4.5

## Executive Summary

Integrate **MiniCPM-o 4.5** (9B param multimodal model) into IRISVOICE to create a
unified **see â†’ hear â†’ think â†’ speak â†’ act** experience. The model natively handles
vision, speech, and text in one end-to-end architecture â€” replacing the current
fragmented pipeline (separate STT â†’ LLM â†’ TTS) with a single omni-model.

---

## ðŸ—ï¸ Current Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IRIS Current Pipeline                                   â”‚
â”‚                                                          â”‚
â”‚  Mic â†’ WakeWord â†’ VAD â†’ STT (Whisper) â†’ LM Studio LLM   â”‚
â”‚                                           â†“              â”‚
â”‚                                     TTS (OpenAI/pyttsx3) â”‚
â”‚                                           â†“              â”‚
â”‚                                      Speaker Output      â”‚
â”‚                                                          â”‚
â”‚  GUI Automation: pyautogui + mss screenshots             â”‚
â”‚  Vision (stub): Anthropic Claude â†’ JSON coords           â”‚
â”‚  GUI Agent:     vision + operator instruction loop       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Files
| Module | File | Purpose |
|--------|------|---------|
| Audio Engine | `backend/audio/engine.py` | Singleton orchestrator: wakeâ†’VADâ†’STTâ†’LLMâ†’TTSâ†’play |
| Audio Pipeline | `backend/audio/pipeline.py` | PyAudio I/O streaming |
| STT Model | `backend/audio/model_manager.py` | LFM 2.5 Audio (Whisper fallback) |
| Conversation | `backend/agent/conversation.py` | LM Studio chat completions |
| TTS | `backend/agent/tts.py` | OpenAI TTS / pyttsx3 / LiquidAI |
| Vision | `backend/automation/vision.py` | Anthropic Claude vision (stub) |
| GUI Operator | `backend/automation/operator.py` | pyautogui + mss screenshots |
| GUI Agent | `backend/automation/vision.py` | Multi-step instruction executor |

---

## ðŸŽ¯ Target Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IRIS Unified Omni Pipeline (MiniCPM-o 4.5)                        â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Mic     â”‚â”€â”€â†’â”‚                                                  â”‚ â”‚
â”‚  â”‚ (audio) â”‚   â”‚                                                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚        MiniCPM-o 4.5  (Ollama)                  â”‚ â”‚
â”‚                â”‚        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  Accepts: images, audio, video, text             â”‚ â”‚
â”‚  â”‚ Screen  â”‚â”€â”€â†’â”‚  Returns: text + audio (TTS) responses           â”‚ â”‚
â”‚  â”‚ (mss)   â”‚   â”‚                                                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  Modes:                                          â”‚ â”‚
â”‚                â”‚    â€¢ Visual Understanding (screenshots)           â”‚ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â€¢ Simplex Omni (audio + vision â†’ text/audio)  â”‚ â”‚
â”‚  â”‚ Webcam  â”‚â”€â”€â†’â”‚    â€¢ Realtime Speech Conversation                â”‚ â”‚
â”‚  â”‚ (opt.)  â”‚   â”‚    â€¢ GUI Agent (screenshot â†’ action plan)        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                                      â”‚
â”‚                              â†“                                      â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚                 â”‚  Response Router       â”‚                          â”‚
â”‚                 â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚                          â”‚
â”‚                 â”‚  â€¢ Text â†’ UI display   â”‚                          â”‚
â”‚                 â”‚  â€¢ Audio â†’ Speaker     â”‚                          â”‚
â”‚                 â”‚  â€¢ Actions â†’ Operator  â”‚                          â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“¦ Deployment Strategy: Ollama (Recommended for Local)

### Why Ollama?
1. **Native Windows support** â€” single binary installer
2. **GGUF quantization** â€” MiniCPM-o 4.5 Q4_K_M fits in ~6GB VRAM
3. **OpenAI-compatible API** â€” drop-in replacement for LM Studio
4. **Image input support** â€” base64 images in API calls
5. **No Python GPU dependencies** â€” no CUDA toolkit, no torch conflicts

### Hardware Requirements
| Quantization | VRAM | Quality | Speed |
|-------------|------|---------|-------|
| Q4_K_M | ~6 GB | Good | Fast |
| Q5_K_M | ~7 GB | Better | Good |
| Q6_K | ~8 GB | Great | Moderate |
| Q8_0 | ~10 GB | Best quantized | Slower |

### Installation Steps
```powershell
# 1. Install Ollama (if not already)
winget install Ollama.Ollama

# 2. Pull MiniCPM-o 4.5 (vision-language model via GGUF)
ollama pull openbmb/minicpm-o4.5
# OR for custom quantization:
# Download from https://huggingface.co/openbmb/MiniCPM-o-4_5-gguf
# Create Modelfile and: ollama create minicpm-o4.5 -f Modelfile

# 3. Verify it works
ollama run openbmb/minicpm-o4.5 "Hello, describe what you can do"
```

---

## ðŸ”§ Implementation Phases

### Phase 1: Vision Provider â€” MiniCPM-o via Ollama API
**Files:** `backend/automation/vision.py`, `backend/automation/__init__.py`

Replace the Anthropic Claude stub with a local MiniCPM-o provider that sends
screenshots as base64 images to Ollama's `/api/generate` endpoint.

```python
# New VisionProvider member
class VisionProvider(Enum):
    ANTHROPIC = "anthropic"
    VOLCENGINE = "volcengine"
    LOCAL = "local"
    MINICPM_OLLAMA = "minicpm_ollama"  # NEW

# Key method signature
async def _detect_with_minicpm(self, screenshot_base64: str, description: str):
    """Use MiniCPM-o 4.5 via Ollama for vision understanding"""
    response = requests.post("http://localhost:11434/api/generate", json={
        "model": "minicpm-o4.5",
        "prompt": f"Find the UI element: '{description}'. Return JSON with x,y,width,height...",
        "images": [screenshot_base64],
        "stream": False
    })
```

### Phase 2: Unified Conversation Manager with Vision Context
**File:** NEW `backend/agent/omni_conversation.py`

A new conversation manager that can include visual context (screenshots) alongside
text prompts. This gives IRIS "eyes" during normal conversation.

```python
class OmniConversationManager:
    """Multimodal conversation via Ollama with MiniCPM-o 4.5"""

    def generate_response(self, user_text: str, screenshot_b64: str = None):
        """Generate response with optional visual context"""
        payload = {
            "model": "minicpm-o4.5",
            "prompt": user_text,
            "stream": False,
        }
        if screenshot_b64:
            payload["images"] = [screenshot_b64]

        response = requests.post(
            f"{self.endpoint}/api/generate",
            json=payload, timeout=60
        )
        return response.json()["response"]
```

### Phase 3: Screen-Aware Audio Engine
**File:** `backend/audio/engine.py` (modify `_run_inference`)

Enhance the inference pipeline so that when the user speaks, IRIS also captures
a screenshot and sends both audio transcript + screenshot to MiniCPM-o.

**Before:**
```
User speaks â†’ STT â†’ text â†’ LM Studio â†’ response text â†’ TTS â†’ speak
```

**After:**
```
User speaks â†’ STT â†’ text â”€â”€â”
                           â”œâ”€â”€â†’ MiniCPM-o (text + image) â†’ response â†’ TTS â†’ speak
Screen capture â†’ base64 â”€â”€â”€â”˜
```

### Phase 4: Enhanced GUI Agent with Local Vision
**File:** `backend/automation/vision.py` (modify `GUIAgent`)

Replace the cloud-dependent GUI agent with fully local MiniCPM-o vision:
- Take screenshot â†’ send to MiniCPM-o â†’ get structured action plan
- Execute actions via `NativeGUIOperator`
- Loop until task complete

### Phase 5: Real-time Screen Monitoring (Proactive Mode)
**File:** NEW `backend/vision/screen_monitor.py`

Periodic screenshot analysis for proactive assistance:
- Configurable interval (e.g., every 5 seconds when active)
- Detect context changes (new window, error dialog, etc.)
- Proactively offer help based on what IRIS "sees"

---

## ðŸ“ New File Structure

```
backend/
â”œâ”€â”€ vision/                          # NEW module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ minicpm_client.py           # Core Ollama + MiniCPM-o client
â”‚   â”œâ”€â”€ screen_capture.py           # Screenshot utilities (from mss)
â”‚   â”œâ”€â”€ screen_monitor.py           # Proactive screen monitoring
â”‚   â””â”€â”€ context_analyzer.py         # Scene understanding & change detection
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ omni_conversation.py        # NEW: vision-aware conversation
â”‚   â””â”€â”€ (existing files unchanged)
â”œâ”€â”€ automation/
â”‚   â”œâ”€â”€ vision.py                   # MODIFIED: add MiniCPM provider
â”‚   â””â”€â”€ (existing files unchanged)
â””â”€â”€ audio/
    â””â”€â”€ engine.py                   # MODIFIED: screenshot during inference
```

---

## ðŸ”Œ New Dependencies

Add to `requirements.txt`:
```
# Vision / MiniCPM-o Integration
Pillow>=10.0.0          # Image processing (already used in operator.py)
mss>=9.0.0              # Screen capture (already used in operator.py)
requests>=2.31.0        # HTTP client for Ollama API (already in deps)
```

> **No new heavy dependencies needed!** Ollama handles model loading externally.
> The integration is purely HTTP API-based, keeping the Python backend lightweight.

---

## ðŸ§ª Implementation Order & Priority

| # | Phase | Priority | Effort | Impact |
|---|-------|----------|--------|--------|
| 1 | MiniCPM-o Ollama Client | ðŸ”´ Critical | 2 hrs | Foundation for everything |
| 2 | Vision-aware conversation | ðŸ”´ Critical | 3 hrs | "IRIS can see" |
| 3 | Screen-aware audio engine | ðŸŸ¡ High | 2 hrs | "Ask about what's on screen" |
| 4 | Local GUI agent | ðŸŸ¡ High | 3 hrs | Replace cloud vision with local |
| 5 | Proactive monitoring | ðŸŸ¢ Nice-to-have | 4 hrs | "Hey, I noticed..." |

---

## ðŸ”€ Integration Points with Existing Code

### 1. `AudioEngine._run_inference()` (engine.py:278-432)
**Change:** Before calling conversation manager, capture screenshot and pass as
context to the new `OmniConversationManager`.

### 2. `VisionModelClient` (vision.py:31-178)
**Change:** Add `MINICPM_OLLAMA` provider with local Ollama API calls.

### 3. `GUIAgent.execute_instruction()` (vision.py:190-239)
**Change:** Use local MiniCPM-o instead of Anthropic for screen analysis.

### 4. `AIConversationManager` (conversation.py)
**Change:** Can stay as fallback. New `OmniConversationManager` takes priority
when MiniCPM-o is available, gracefully falling back to LM Studio text-only.

### 5. Frontend: New subnode configuration
**Change:** Add "Vision" settings under the AUTOMATE category hexagonal node:
- Toggle: "Screen awareness" (on/off)
- Dropdown: "Vision model" (MiniCPM-o / Claude / Disabled)
- Slider: "Screen capture interval" (1-30 seconds)
- Toggle: "Proactive mode" (on/off)

---

## ðŸŽ­ Immersive Experience Features

### "See & Describe" Mode
User says: *"What am I looking at?"*
â†’ IRIS captures screen â†’ MiniCPM-o analyzes â†’ speaks description

### "Help Me With This" Mode
User says: *"Help me fill out this form"*
â†’ IRIS captures screen â†’ understands the form â†’ guides user step by step

### "Watch & Alert" Mode
IRIS periodically captures screen â†’ detects important changes
â†’ *"Hey, looks like you got a new email from your boss"*

### "Do This For Me" Mode (GUI Agent)
User says: *"Open Chrome and search for Python tutorials"*
â†’ IRIS uses vision + operator to execute multi-step task autonomously

---

## âš¡ Performance Optimization

1. **Screenshot caching** â€” Don't re-capture if screen hasn't changed (pixel diff)
2. **Resolution scaling** â€” Downscale screenshots to 1024px width before sending
3. **Prompt caching** â€” Ollama keeps model in memory between requests
4. **Async pipeline** â€” Screenshot capture happens in parallel with STT
5. **Lazy loading** â€” Only initialize vision module when first needed
6. **Batch mode** â€” Group multiple vision queries into single context

---

## ðŸš€ Getting Started â€” Next Steps

1. **Install Ollama** and pull `openbmb/minicpm-o4.5`
2. Implement Phase 1: `backend/vision/minicpm_client.py`
3. Wire it into the audio engine (Phase 3)
4. Test the "What am I looking at?" flow end-to-end
5. Add UI controls for vision settings

Ready to start implementing? Say the word and we'll begin with Phase 1! ðŸŽ¯
